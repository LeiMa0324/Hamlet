DATA PROCESS:
    1. choose 10 most frequent ticks as the kleene event types
    2. replicate the records *60, divide the vol/60
    so for each timestamp we have a burst with size 60

    3. choose 100 least frequent ticks as the prefix/suffix event types in the workload
    4. replicate the record(only 1) *20, divide the vol/20

    5. choose the 10-20th most frequent ticks as the irrelevant events in the stream
    6. replicate 60, vol/60

521860 records
    50% relevant events:
        kleene events
        prefix/suffix events

    50% irrelevant events

    7. add a column "open_level" = {low, mid, high} as the groupby column


HAMLET SYSTEM

Attribute:
    an column in a dataset

DatasetSchema:
    an array of attributes

Query:
    Class of a query
    members:
        schema
        pattern
        predicate
        aggregation function
        groupby
        window

QueryParser:
    Given the text of a query, parse it into a Query instance

WorkloadTemplate:
    parameters: query number, number of mini-workload

    miniworkload rules:
        same event type, same groupby
        compatible aggregation
        predicates on same event type and attribute
        compatible windows

Workload:
    the class of workload
    members:
        a list of  Queries
        schema

WorkloadAnalyzer:
    1. given a text file of workload, parse it into a workload
    2. analyze the workload, partition the queries into mini-workloads


DOING NOW:
StreamLoader:
    filter out the irrelevant events(almost half of it)

StreamPartitioner:
    parameters: mini-workloads, a stream
    partition the stream into sub-streams for each mini-workload











