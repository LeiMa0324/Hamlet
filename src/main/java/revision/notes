DATA PROCESS:
    1. choose 10 most frequent ticks as the kleene event types
    2. replicate the records *60, divide the vol/60
    so for each timestamp we have a burst with size 60

    3. choose 100 least frequent ticks as the prefix/suffix event types in the workload
    4. replicate the record(only 1) *20, divide the vol/20

    5. choose the 10-20th most frequent ticks as the irrelevant events in the stream
    6. replicate 60, vol/60

521860 records
    50% relevant events:
        kleene events
        prefix/suffix events

    50% irrelevant events

    7. add a column "open_level" = {low, mid, high} as the groupby column


HAMLET SYSTEM

Attribute:
    an column in a dataset

DatasetSchema:
    an array of attributes

Query:
    Class of a query
    members:
        schema
        pattern
        predicate
        aggregation function
        groupby
        window

QueryParser:
    Given the text of a query, parse it into a Query instance

WorkloadTemplate:
    parameters: query number, number of mini-workload

    miniworkload rules:
        same event type, same groupby
        compatible aggregation
        predicates on same event type and attribute
        compatible windows

Workload:
    the class of workload
    members:
        a list of  Queries
        schema

WorkloadAnalyzer:
    1. given a text file of workload, parse it into a workload
    2. analyze the workload, partition the queries into mini-workloads


StreamLoader:
    filter out the irrelevant lines of the whole workload
    package the relevant data into an event

StreamPartitioner:
    parameters: a mini-workload, a stream
    second filter of events, filter out the irrelevant events of the mini-workload
    since we group by the kleene.open_level, the not kleene events are duplicated to every substream
    the kleene events with the correct open_level value go to the corresponding substream

Executor:

    BurstLoader:
        load the events into burst, apply the predicates to events

    Event:
        count(prefix events)
        or
        snapshot expression(Kleene or suffix events)

    PredicateManager
        provides the methods of filtering events by predicates

    PredecessorManager
        find the predecessors of an event

    WindowManager
        init, slide, expire windows

    CountManager
        provides the methods of count calculation of a single event.
        including sum counts or sum snapshots into an expression

            -KleeneEventCountManager
                same predecessors for all the queries: get the expression of snapshots for all queries
                different predecessors for all the queries: evaluate for each query, create an event-level snapshot

            -NoneKleeneEventCountManager
                for prefix: count =1
                for suffix: add the snapshots of the predecessors together to a new expression of snapshots

    SnapshotManager:
        manages the snapshots, update, insert, search

    GraphletManager:
        manages the graphlets, search the graphlets

    Graphlet:
        a list of events
        total count of all events(for quick prefix event counting)

        propagate the count or snapshot expressions among the events

    Graph:
        static Graph

        Kleene burst:
            if could be merged with last kleene graphlet, then merge
                   condition: no prefix or suffix(???) graphlet after last kleene graphlet
            if not
                create new kleene graphlet, new g-level snapshot

            propagate snapshots

        None Kleene burst:
            prefix: propagate counts = 1
            suffix: propagate snapshots


STATIC GRAPHLET:
    AGGREGATOR: COUNT, SUM, AVG
    WINDOWS:
        TIME BASED, 5 MIN, 10 MIN, 15 MIN, 20 MIN
        BUT ONLY CHECK EXPIRED AFTER A WHOLE GRAPHLET



todo:
 1. analyzer overhead(just value)
 2. dynamic decision(chart)
 	average over burst size
 3. snapshot maintenance overhead(chart)

 outputs: 1. actual values
          2. system performance

new exp:
 show overhead of different components of greta vs. dynamic hamlet
 break cost into different components


double check rebuttal

Figure 17:
    x-axis

    epw
    # of queries


Latency: same execution time
Throughput: avg #events per second
peak memory: maximal memory to store snapshots



29. Oct
Dynamic is done, count is the same with static, sum is slightly different
expand dataset from 50k to 100k
generate workload size from 50-100

Experiment setup:
    vary #queries 50-100
            50, 60, 70, 80 ,90, 100

    vary epw 50-100k
            50k, 60k, 70k, 80k, 90k, 100k

    calculate memory, latency, throughput, logging



logging:
statistics of size of merged graphlets
avg size of merged graphlets
max, min

# of graphlets
# of merged graphlets


PART1 : STATIC VS. DYNAMIC
Execution stack bar chart: vertically
dynamic: optimizer, and executor
static: executor

manipulate the event snapshots in a burst
for 30% probability, a burst will have event snapshot for every event in it.

    exp1 vary epw 50k -100k
    exp2 vary workload size 50 - 100

    add patterns, dynamic opt darker line above exe

 how to increase workload(20-100) increase, stable, drop
 predicates



PART2 : OVERHEAD ANALYSIS
Overhead(over # queries)
    Static OVERHEAD
    1. static workload analyzer AND template generation


    For dynamic execution:
    3. sum of dynamic optimizer
    4. sum of execution
    check the sum of these two


